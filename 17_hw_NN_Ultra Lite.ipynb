{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBvuSQtdCyPq"
   },
   "source": [
    "### Задание № 17 \n",
    "\n",
    "\n",
    "#### Тема: Генерация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание Ultra Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqZpzs2nBs63"
   },
   "source": [
    "Перепишите сеть для генерации текста с нуля в новом ноутбуке. Можно подсматривать в ноутбук занятия, но крайне желательно писать код своими руками, а не копировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание Lite\n",
    "\n",
    "\n",
    "Используя три любых простых вопроса, сравните ответы сети на них на разной степени натренированности:\n",
    "\n",
    "1. 20 эпох – удается ли боту отвечать целыми словами?\n",
    "\n",
    "2. + 30 эпох на этой же сетке и с этими же вопросами – появился ли прогресс в качестве ответа сети (ответ целыми предложениями разумной длины)?\n",
    "\n",
    "3. Ещё + 50 эпох – удается ли сети выдавать ответы, “похожие на правду”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание Pro\n",
    "\n",
    "\n",
    "Попробуйте улучшить текущий скрипт чат-бота, внедрив блок кода для присвоения словам вне словаря (out-of-vocabulary) метки «unknown» так, чтобы, встретив в запросе незнакомое слово, исполнение кода не останавливалось, а продолжалось, игнорируя «unknown» слова.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание Ultra pro\n",
    "\n",
    "\n",
    "Скачайте свои переписки/диалоги из соц.сетей/мессенджеров и сформируйте свою диалоговую базу. На основе этой базы обучите модель, построив тем самым чат бота, генерирующего текст ответов в вашем стиле.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLX9aj3eFCPB"
   },
   "source": [
    "#### 1. Импорт библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "3j1Wpkvc3Q2s"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.models import Model, load_model # абстрактный класс базовой модели, метод загрузки предобученной модели\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input # необходимые слои для нейросети\n",
    "from tensorflow.keras.optimizers import RMSprop, Adadelta # оптимизатор\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # метод ограничения последовательности заданной длиной\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # токенизатор кераса для обработки текста\n",
    "from tensorflow.keras import utils # утилиты кераса для one hot кодировки\n",
    "from tensorflow.keras.utils import plot_model # удобный график для визуализации архитектуры модели\n",
    "import yaml # модуль для удобной работы с файлами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('/Users/ekaterina/Desktop/LEARN/IT/УИИ/Введение в нейронные сети/17_Диалоги(рассказы)_censored.yml', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wxdi0Fqeg1LH"
   },
   "source": [
    "#### 2. Парсинг данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEA8TR_oerov",
    "outputId": "2e410dd6-8545-4dd0-f190-ba0925d4fc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пар вопрос-ответ : 11893\n",
      "Пример диалога : ['Перезалил?', 'Да вроде бы нет...']\n"
     ]
    }
   ],
   "source": [
    "document = yaml.safe_load(corpus) \n",
    "conversations = document['разговоры']  \n",
    "print('Количество пар вопрос-ответ : {}'.format(len(conversations)))\n",
    "print('Пример диалога : {}'.format(conversations[123]))\n",
    "corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYg8z8Vj76bu",
    "outputId": "af2cabcb-be88-4a49-ef75-905e8d827896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос : Около сотни...\n",
      "Ответ : <START> Точнее! <END>\n"
     ]
    }
   ],
   "source": [
    "questions = list() \n",
    "answers = list() \n",
    "\n",
    "for con in conversations: # для каждой пары вопрос-ответ\n",
    "  if len(con) > 2 : # если ответ содержит более двух предложений (кол-во реплик, кол-во вариантов ответа)\n",
    "    questions.append(con[0]) # то вопросительную реплику отправляем в список вопросов\n",
    "    replies = con[1:] # а ответную составляем из последующих строк\n",
    "    ans = '' # здесь соберем ответ\n",
    "    for rep in replies: # каждую реплику в ответной реплике\n",
    "      ans += ' ' + rep \n",
    "    answers.append(ans) #добавим в список ответов\n",
    "  elif len(con)> 1: # если на 1 вопрос приходится 1 ответ\n",
    "    questions.append(con[0]) # то вопросительную реплику отправляем в список вопросов\n",
    "    answers.append(con[1]) # а ответную в список ответов\n",
    "\n",
    "# Очищаем строки с неопределенным типом ответов\n",
    "answersCleaned = list()\n",
    "for i in range(len(answers)):\n",
    "  if type(answers[i]) == str:\n",
    "    answersCleaned.append(answers[i]) #если тип - строка, то добавляем в ответы\n",
    "  else:\n",
    "    questions.pop(i) # если не строка, то ответ не добавился, и плюс убираем соответствующий вопрос\n",
    "\n",
    "answers = list()\n",
    "for i in range(len(answersCleaned)):\n",
    "  answers.append( '<START> ' + answersCleaned[i] + ' <END>' )\n",
    "\n",
    "# Выведем обновленные данные на экран\n",
    "print('Вопрос : {}'.format(questions[200]))\n",
    "print('Ответ : {}'.format(answers[200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvn1jvRd9tep",
    "outputId": "ddd4d515-c573-4d4e-e6d5-7f78d46804f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фрагмент словаря : [('start', 1), ('end', 2), ('что', 3), ('не', 4), ('я', 5), ('а', 6), ('ты', 7), ('это', 8), ('да', 9), ('в', 10), ('нет', 11), ('как', 12), ('и', 13), ('вы', 14), ('ну', 15), ('с', 16), ('на', 17), ('же', 18), ('так', 19), ('он', 20), ('у', 21), ('кто', 22), ('где', 23), ('все', 24), ('мы', 25), ('то', 26), ('мне', 27), ('тебя', 28), ('меня', 29), ('здесь', 30), ('еще', 31), ('почему', 32), ('о', 33), ('тебе', 34), ('там', 35), ('есть', 36), ('его', 37), ('за', 38), ('куда', 39), ('вот', 40), ('ничего', 41), ('вас', 42), ('знаю', 43), ('чем', 44), ('но', 45), ('она', 46), ('они', 47), ('ли', 48), ('чего', 49), ('вам', 50)]\n",
      "Размер словаря : 15092\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers) # загружаем в токенизатор список вопросов-ответов для сборки словаря частотности\n",
    "vocabularyItems = list(tokenizer.word_index.items()) # список с cодержимым словаря\n",
    "vocabularySize = len(vocabularyItems)+1 # размер словаря\n",
    "print( 'Фрагмент словаря : {}'.format(vocabularyItems[:50]))\n",
    "print( 'Размер словаря : {}'.format(vocabularySize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsUqzEBXg9Mu"
   },
   "source": [
    "#### 3. Подготовка выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p4nNBJUQgebF",
    "outputId": "1ca5e324-f38c-4f13-ec50-76035bb640a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример оригинального вопроса на вход : Какая же мораль?\n",
      "Пример кодированного вопроса на вход : [ 170   18 5703    0    0    0    0    0    0    0    0]\n",
      "Размеры закодированного массива вопросов на вход : (11888, 11)\n",
      "Установленная длина вопросов на вход : 11\n"
     ]
    }
   ],
   "source": [
    "tokenizedQuestions = tokenizer.texts_to_sequences(questions) # разбиваем текст вопросов на последовательности индексов\n",
    "maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) # уточняем длину самого большого вопроса\n",
    "# Делаем последовательности одной длины, заполняя нулями более короткие вопросы\n",
    "paddedQuestions = pad_sequences(tokenizedQuestions, maxlen=maxLenQuestions, padding='post')\n",
    "\n",
    "# Предподготавливаем данные для входа в сеть\n",
    "encoderForInput = paddedQuestions\n",
    "print('Пример оригинального вопроса на вход : {}'.format(questions[100])) \n",
    "print('Пример кодированного вопроса на вход : {}'.format(encoderForInput[100])) \n",
    "print('Размеры закодированного массива вопросов на вход : {}'.format(encoderForInput.shape)) \n",
    "print('Установленная длина вопросов на вход : {}'.format(maxLenQuestions)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tjvhMuzqFJD",
    "outputId": "0b9a2c0d-a10e-46db-9935-dbc67b282b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример оригинального ответа на вход: <START> Никакой. Так просто вспомнилось. <END>\n",
      "Пример раскодированного ответа на вход : [   1 1743    2    0    0    0    0    0    0    0    0    0    0]\n",
      "Размеры раскодированного массива ответов на вход : (11888, 13)\n",
      "Установленная длина ответов на вход : 13\n"
     ]
    }
   ],
   "source": [
    "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов\n",
    "maxLenAnswers = max([len(x) for x in tokenizedAnswers]) # уточняем длину самого большого ответа\n",
    "# Делаем последовательности одной длины, заполняя нулями более короткие ответы\n",
    "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers, padding='post')\n",
    "\n",
    "# Предподготавливаем данные для входа в сеть\n",
    "decoderForInput = paddedAnswers # переводим в numpy массив\n",
    "print('Пример оригинального ответа на вход: {}'.format(answers[100])) \n",
    "print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[200])) \n",
    "print('Размеры раскодированного массива ответов на вход : {}'.format(decoderForInput.shape)) \n",
    "print('Установленная длина ответов на вход : {}'.format(maxLenAnswers)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "MwsKk9dzNeqI"
   },
   "outputs": [],
   "source": [
    "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов\n",
    "for i in range(len(tokenizedAnswers)) : # для разбитых на последовательности ответов\n",
    "  tokenizedAnswers[i] = tokenizedAnswers[i][1:] # избавляемся от тега <START>\n",
    "# Делаем последовательности одной длины, заполняя нулями более короткие ответы\n",
    "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers , padding='post')\n",
    "\n",
    "oneHotAnswers = utils.to_categorical(paddedAnswers, vocabularySize) # переводим в one hot vector\n",
    "decoderForOutput = np.array(oneHotAnswers) # и сохраняем в виде массива numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fRl1k7SVaA6w",
    "outputId": "278382e5-2279-4602-d1a7-f9b92c943875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример раскодированного ответа на вход : [    1   672    19    93 10547     2     0     0     0     0     0     0\n",
      "     0]\n",
      "Пример раскодированного ответа на выход : [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Размеры раскодированного массива ответов на выход : (11888, 13, 15092)\n",
      "Установленная длина вопросов на выход : 13\n"
     ]
    }
   ],
   "source": [
    "print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[100][:21]))  \n",
    "print('Пример раскодированного ответа на выход : {}'.format(decoderForOutput[100][4][:21])) \n",
    "print('Размеры раскодированного массива ответов на выход : {}'.format(decoderForOutput.shape))\n",
    "print('Установленная длина вопросов на выход : {}'.format(maxLenAnswers)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0KR6Mh_hp1f"
   },
   "source": [
    "#### 4. Параметры нейросети и модель обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "1rRKDr4rhXcZ"
   },
   "outputs": [],
   "source": [
    "encoderInputs = Input(shape=(None , )) # размеры на входе сетки (здесь будет encoderForInput)\n",
    "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
    "encoderEmbedding = Embedding(vocabularySize, 200 , mask_zero=True) (encoderInputs)\n",
    "# Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c\n",
    "# Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже\n",
    "encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)\n",
    "encoderStates = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "K_yv8Y6QWX2D"
   },
   "outputs": [],
   "source": [
    "decoderInputs = Input(shape=(None, )) # размеры на входе сетки (здесь будет decoderForInput)\n",
    "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
    "# mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: \"У меня все хорошо PAD PAD PAD PAD PAD PAD..\"\n",
    "decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) \n",
    "# Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c\n",
    "decoderLSTM = LSTM(200, return_state=True, return_sequences=True)\n",
    "decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)\n",
    "# И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе\n",
    "decoderDense = Dense(vocabularySize, activation='softmax') \n",
    "output = decoderDense (decoderOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 891
    },
    "id": "KYnTen_UWc5F",
    "outputId": "178bda1e-478d-4470-9eb8-090e8335f844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 200)    3018400     input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 200)    3018400     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 200), (None, 320800      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, None, 200),  320800      embedding_7[0][0]                \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 15092)  3033492     lstm_7[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 9,711,892\n",
      "Trainable params: 9,711,892\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoderInputs, decoderInputs], output)\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "print(model.summary()) # выведем на экран информацию о построенной модели нейросети\n",
    "plot_model(model, to_file='model.png') # и построим график для визуализации слоев и связей между ними"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbelEm0zhadD",
    "outputId": "f243f267-a84c-453f-e322-a08fff97ca66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "238/238 [==============================] - 70s 259ms/step - loss: 2.2137\n",
      "Epoch 2/2\n",
      "238/238 [==============================] - 69s 290ms/step - loss: 1.9739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6fe616850>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Запустим обучение и сохраним модель https://www.youtube.com/watch?v=JSPbJ9CNZ9w&t=1348s\n",
    "model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=2) \n",
    "#model.save( '/content/drive/My Drive/Предобученные сети/model_100epochs(rms).h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "iEHzJjlRYK5-"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adadelta(), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rPD8f5tY4up",
    "outputId": "096e75c4-edbd-4409-b645-a9a729a78280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "238/238 [==============================] - 64s 237ms/step - loss: 1.9122\n",
      "Epoch 2/10\n",
      "238/238 [==============================] - 55s 233ms/step - loss: 1.9120\n",
      "Epoch 3/10\n",
      "238/238 [==============================] - 55s 231ms/step - loss: 1.9119\n",
      "Epoch 4/10\n",
      "238/238 [==============================] - 56s 233ms/step - loss: 1.9117\n",
      "Epoch 5/10\n",
      "238/238 [==============================] - 60s 253ms/step - loss: 1.9116\n",
      "Epoch 6/10\n",
      "238/238 [==============================] - 58s 242ms/step - loss: 1.9115\n",
      "Epoch 7/10\n",
      "238/238 [==============================] - 58s 242ms/step - loss: 1.9114\n",
      "Epoch 8/10\n",
      "238/238 [==============================] - 57s 240ms/step - loss: 1.9113\n",
      "Epoch 9/10\n",
      "238/238 [==============================] - 58s 243ms/step - loss: 1.9112\n",
      "Epoch 10/10\n",
      "238/238 [==============================] - 57s 239ms/step - loss: 1.9111\n"
     ]
    }
   ],
   "source": [
    "# Запустим обучение и сохраним модель\n",
    "model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=10) \n",
    "model.save( '/Users/ekaterina/Desktop/LEARN/IT/УИИ/Введение в нейронные сети/17_model_100epochs(rms) + 50(ada).h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "6jrhHhh02_uO"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/Users/ekaterina/Desktop/LEARN/IT/УИИ/Введение в нейронные сети/17_model_100epochs(rms) + 50(ada).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_U_rY8UiRL2"
   },
   "source": [
    "#### 5. Подготовка и запуск рабочей нейросети с генерацией ответов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Kv9utvcjh2co"
   },
   "outputs": [],
   "source": [
    "def makeInferenceModels():\n",
    "  # Определим модель кодера, на входе далее будут закодированные вопросы(encoderForInputs), на выходе состояния state_h, state_c\n",
    "  encoderModel = Model(encoderInputs, encoderStates) \n",
    "\n",
    "  decoderStateInput_h = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_h\n",
    "  decoderStateInput_c = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_c\n",
    "\n",
    "  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] # возьмем оба inputs вместе и запишем в decoderStatesInputs\n",
    "\n",
    "  # Берём ответы, прошедшие через эмбединг, вместе с состояниями и подаём LSTM cлою\n",
    "  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)\n",
    "  decoderStates = [state_h, state_c] # LSTM даст нам новые состояния\n",
    "  decoderOutputs = decoderDense(decoderOutputs) # и ответы, которые мы пропустим через полносвязный слой с софтмаксом\n",
    "\n",
    "  # Определим модель декодера, на входе далее будут раскодированные ответы (decoderForInputs) и состояния\n",
    "  # на выходе предсказываемый ответ и новые состояния\n",
    "  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)\n",
    "\n",
    "  return encoderModel , decoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "dSSOhZpgh9LI"
   },
   "outputs": [],
   "source": [
    "def strToTokens(sentence: str): # функция принимает строку на вход (предложение с вопросом)\n",
    "  words = sentence.lower().split() # приводит предложение к нижнему регистру и разбирает на слова\n",
    "  tokensList = list() # здесь будет последовательность токенов/индексов\n",
    "  for word in words: # для каждого слова в предложении\n",
    "    tokensList.append(tokenizer.word_index[word]) # определяем токенизатором индекс и добавляем в список\n",
    "\n",
    "    # Функция вернёт вопрос в виде последовательности индексов, ограниченной длиной самого длинного вопроса из нашей базы вопросов\n",
    "  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJ0Dxd1eiEid",
    "outputId": "35e60230-c6f3-4063-c1ff-6b0d8d7cfc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Задайте вопрос : который час\n",
      " да \n",
      "Задайте вопрос : как дела\n",
      " да \n",
      "Задайте вопрос : что делаешь\n",
      " да \n"
     ]
    }
   ],
   "source": [
    "encModel , decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера\n",
    "\n",
    "for _ in range(3): # задаем количество вопросов, и на каждой итерации в этом диапазоне:\n",
    "  # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом\n",
    "  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))\n",
    "  # Создаём пустой массив размером (1, 1)\n",
    "  emptyTargetSeq = np.zeros((1, 1))    \n",
    "  emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса\n",
    "\n",
    "  stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова\n",
    "  decodedTranslation = '' # здесь будет собираться генерируемый ответ\n",
    "  while not stopCondition : # пока не сработало стоп-условие\n",
    "    # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.\n",
    "    # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния\n",
    "    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
    "    \n",
    "    #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве\n",
    "    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.\n",
    "    sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык\n",
    "    for word , index in tokenizer.word_index.items():\n",
    "      if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря\n",
    "        decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ \n",
    "        sampledWord = word # выбранное слово фиксируем в переменную sampledWord\n",
    "    \n",
    "    # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа\n",
    "    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
    "      stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию\n",
    "\n",
    "\n",
    "    emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова\n",
    "    statesValues = [h, c] # и состояния, обновленные декодером\n",
    "    # и продолжаем цикл с обновленными параметрами\n",
    "  \n",
    "  print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Генерация текста",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
